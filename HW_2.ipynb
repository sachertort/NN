{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Домашнее задачние №2 (CNN)"
      ],
      "metadata": {
        "id": "qzV1DlFBrhrJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZU9pXZIL6If",
        "outputId": "75786e18-ebb0-4895-b147-7ceac4c6c096"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from nltk import word_tokenize\n",
        "from string import punctuation\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.optim as optim\n",
        "from torchmetrics import F1\n",
        "from torchmetrics.functional import f1, recall\n",
        "import ipdb\n",
        "from gensim.models import fasttext \n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gensim==3.8.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAQcMm_djW0S",
        "outputId": "ebda0838-b8b1-454d-9fb8-cc759f77243a"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim==3.8.0 in /usr/local/lib/python3.7/dist-packages (3.8.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.0) (1.4.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.0) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.0) (1.19.5)\n",
            "Requirement already satisfied: smart-open>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.0) (5.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "yzf8QDczrsIw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "soQgJqCAL6Ii"
      },
      "outputs": [],
      "source": [
        "negative = pd.read_csv('negative.csv', sep=';', usecols=[3,4], names=['text', 'class'])\n",
        "positive = pd.read_csv('positive.csv', sep=';', usecols=[3,4], names=['text', 'class'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "Tp4ZZQlML6Ii"
      },
      "outputs": [],
      "source": [
        "dataset = pd.concat([negative, positive])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "L5wuTTpnL6Ij"
      },
      "outputs": [],
      "source": [
        "def simple_preprocess(text):\n",
        "    return [token.lower() for token in word_tokenize(text) if token not in punctuation]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "TChcs3YrL6Ij"
      },
      "outputs": [],
      "source": [
        "dataset['tokenized'] = dataset['text'].apply(lambda t: simple_preprocess(t))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "_K6ub-D1L6Ij"
      },
      "outputs": [],
      "source": [
        "dataset['class'] = dataset['class'].apply(lambda c: 0 if c == -1 else 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j25gujV-L6Ik",
        "outputId": "6f3c519e-69bf-4631-ca23-22244132a847"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "всего уникальных слов: 323845\n"
          ]
        }
      ],
      "source": [
        "voc = Counter()\n",
        "for words in dataset['tokenized']:\n",
        "    voc.update(words)\n",
        "print('всего уникальных слов:', len(voc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QvssRSBL6Il",
        "outputId": "641f4133-5227-4576-eb12-dd9be9919b85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "уникальных слов, вcтретившихся больше 5 раз: 31739\n"
          ]
        }
      ],
      "source": [
        "filtered = set()\n",
        "for word in voc:\n",
        "     if voc[word] > 5:\n",
        "        filtered.add(word)\n",
        "print('уникальных слов, вcтретившихся больше 5 раз:', len(filtered))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "jkE0jKGXL6Il"
      },
      "outputs": [],
      "source": [
        "word2id = {'PAD': 0}\n",
        "for word in filtered:\n",
        "    word2id[word] = len(word2id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "VurP1lLZL6In"
      },
      "outputs": [],
      "source": [
        "id2word = {i:word for word, i in word2id.items()}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "DEVICE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqcu_4CoMh1K",
        "outputId": "e3685082-f6fd-4525-c9ba-ce0c52810252"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "hJ_rsxwsL6In"
      },
      "outputs": [],
      "source": [
        "class TwitterDataset(Dataset):\n",
        "    def __init__(self, dataset, word2id, DEVICE):\n",
        "        self.dataset = dataset['tokenized'].values\n",
        "        self.word2id = word2id\n",
        "        self.length = dataset.shape[0]\n",
        "        self.target = dataset['class'].values\n",
        "        self.device = DEVICE\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index): \n",
        "        words = self.dataset[index]\n",
        "        ids = torch.LongTensor([self.word2id[word] for word in words if word in self.word2id])\n",
        "        y = [self.target[index]]\n",
        "        return ids, y\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        ids, y = list(zip(*batch))\n",
        "        padded_ids = pad_sequence(ids, batch_first=True).to(self.device)\n",
        "        y = torch.Tensor(y).to(self.device)\n",
        "        return padded_ids, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "e_4YgSc1L6Ir"
      },
      "outputs": [],
      "source": [
        "train, test = train_test_split(dataset, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "W2ZxT8r8L6Is"
      },
      "outputs": [],
      "source": [
        "train_dataset = TwitterDataset(train, word2id, DEVICE)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_iterator = DataLoader(train_dataset, collate_fn = train_dataset.collate_fn, sampler=train_sampler, batch_size=1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "xRV8HVtcL6Is"
      },
      "outputs": [],
      "source": [
        "test_dataset = TwitterDataset(test, word2id, DEVICE)\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_iterator = DataLoader(test_dataset, collate_fn = test_dataset.collate_fn, sampler=test_sampler, batch_size=1024)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. CNN for words"
      ],
      "metadata": {
        "id": "j9prr3Zgrx2L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "RbAPYTgwL6Iw"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.bigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=2, padding='same')\n",
        "        self.trigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=80, kernel_size=3, padding='same')\n",
        "        self.lastconv = nn.Conv1d(in_channels=180, out_channels=150, kernel_size=2, padding='same')\n",
        "        self.hidden = nn.Linear(in_features=150, out_features=1)\n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, word):\n",
        "        embedded = self.embedding(word)\n",
        "        embedded = embedded.transpose(1,2)\n",
        "        feature_map_bigrams = self.bigrams(embedded)\n",
        "        feature_map_trigrams = self.trigrams(embedded)\n",
        "        concat = torch.cat((feature_map_bigrams, feature_map_trigrams), 1)\n",
        "        feature_map_last = self.lastconv(concat)\n",
        "        pooling = feature_map_last.max(2)[0]\n",
        "        logits = self.hidden(pooling) \n",
        "        logits = self.out(logits)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "wKV4QoWHL6Ix"
      },
      "outputs": [],
      "source": [
        "def training(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0 \n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for i, (texts, ys) in enumerate(iterator):\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(texts)\n",
        "        loss = criterion(preds, ys)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return  epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "wGe5ZiV7L6Iy"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_metric = 0\n",
        "    model.eval() \n",
        "    with torch.no_grad():\n",
        "        for i, (texts, ys) in enumerate(iterator):   \n",
        "            preds = model(texts)\n",
        "            loss = criterion(preds, ys)\n",
        "            epoch_loss += loss.item()\n",
        "            batch_metric = f1(preds.round().long(), ys.long(), ignore_index=0)\n",
        "            epoch_metric += batch_metric.item()\n",
        "        \n",
        "    return epoch_metric / len(iterator), epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "naRUxaBZL6Iz"
      },
      "outputs": [],
      "source": [
        "model = CNN(len(word2id), 200)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "criterion = nn.BCELoss()  \n",
        "\n",
        "model = model.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQqrub6LL6Iz",
        "outputId": "804bf2f8-e37c-4b88-8e3e-57e17cd241a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 0\n",
            "epoch loss on train: 0.5776815804537763\n",
            "\n",
            "test evaluation:\n",
            "epoch loss on test: 0.5407159686088562\n",
            "F1 on test: 0.7172049668100146\n",
            "\n",
            "epoch 1\n",
            "epoch loss on train: 0.5041556649663476\n",
            "\n",
            "test evaluation:\n",
            "epoch loss on test: 0.5074664976861741\n",
            "F1 on test: 0.7464439895417955\n",
            "\n",
            "epoch 2\n",
            "epoch loss on train: 0.45719039557355173\n",
            "\n",
            "test evaluation:\n",
            "epoch loss on test: 0.4889123499393463\n",
            "F1 on test: 0.7568130996492174\n",
            "\n",
            "epoch 3\n",
            "epoch loss on train: 0.4178060271431891\n",
            "\n",
            "test evaluation:\n",
            "epoch loss on test: 0.48286848862965903\n",
            "F1 on test: 0.7594486660427517\n",
            "\n",
            "epoch 4\n",
            "epoch loss on train: 0.38079599734772457\n",
            "\n",
            "test evaluation:\n",
            "epoch loss on test: 0.48223473164770336\n",
            "F1 on test: 0.7655813998646206\n"
          ]
        }
      ],
      "source": [
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(5):\n",
        "    print(f'\\nepoch {i}')\n",
        "    epoch_loss = training(model, train_iterator, optimizer, criterion)\n",
        "    print('epoch loss on train:', epoch_loss)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\ntest evaluation:')\n",
        "    f1_on_test, epoch_loss_on_test = evaluate(model, test_iterator, criterion)\n",
        "    print('epoch loss on test:', epoch_loss_on_test)\n",
        "    print('F1 on test:', f1_on_test)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Улучшение: добавлен Dropout."
      ],
      "metadata": {
        "id": "l-xHWJcbBjAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_DO(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.bigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=2, padding='same')\n",
        "        self.trigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=200, kernel_size=3, padding='same')\n",
        "        self.lastconv = nn.Conv1d(in_channels=300, out_channels=200, kernel_size=2, padding='same')\n",
        "        self.hidden = nn.Linear(in_features=200, out_features=1)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, word):\n",
        "        embedded = self.embedding(word)\n",
        "        embedded = embedded.transpose(1,2)\n",
        "        feature_map_bigrams = self.dropout(self.bigrams(embedded))\n",
        "        feature_map_trigrams = self.dropout(self.trigrams(embedded))\n",
        "        concat = torch.cat((feature_map_bigrams, feature_map_trigrams), 1)\n",
        "        feature_map_last = self.dropout(self.lastconv(concat))\n",
        "        pooling = feature_map_last.max(2)[0]\n",
        "        logits = self.hidden(pooling) \n",
        "        logits = self.out(logits)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "CuExbpkAAKl_"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN_DO(len(word2id), 200)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "criterion = nn.BCELoss()  \n",
        "\n",
        "model = model.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)"
      ],
      "metadata": {
        "id": "oEmhjWxpBEES"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(5):\n",
        "    print(f'\\nepoch {i}')\n",
        "    epoch_loss = training(model, train_iterator, optimizer, criterion)\n",
        "    print('epoch loss on train:', epoch_loss)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\ntest evaluation:')\n",
        "    f1_on_test, epoch_loss_on_test = evaluate(model, test_iterator, criterion)\n",
        "    print('epoch loss on test:', epoch_loss_on_test)\n",
        "    print('F1 on test:', f1_on_test)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "fiC6OL9RAMMm",
        "outputId": "a4036485-b6b0-415a-9252-1dd086c82c7a"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-185-612d2679f2dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\nepoch {i}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch loss on train:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-155-983d2f6a479d>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(model, iterator, optimizer, criterion)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# ставим модель в обучение, явно указываем, что сейчас надо будет хранить градиенты у всех весов\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#итерируемся по батчам\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#обнуляем градиенты\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#прогоняем данные через модель\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1. CNN for words with pretrained embeddings"
      ],
      "metadata": {
        "id": "YLeAkUrWr91Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mVGdPfsL6I0",
        "outputId": "be700afc-e4dc-4546-fb46-060392db4232"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-10 20:28:30--  http://vectors.nlpl.eu/repository/20/214.zip\n",
            "Resolving vectors.nlpl.eu (vectors.nlpl.eu)... 129.240.189.181\n",
            "Connecting to vectors.nlpl.eu (vectors.nlpl.eu)|129.240.189.181|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1920218982 (1.8G) [application/zip]\n",
            "Saving to: ‘214.zip.1’\n",
            "\n",
            "214.zip.1            52%[=========>          ] 952.43M  22.7MB/s    eta 41s    ^C\n"
          ]
        }
      ],
      "source": [
        "!wget http://vectors.nlpl.eu/repository/20/214.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip 214.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bBjFwrCf_Z_",
        "outputId": "62ff8704-3f66-4cd3-e0df-6f951b2c0e7e"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  214.zip\n",
            "replace meta.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ft = fasttext.FastTextKeyedVectors.load('model.model')"
      ],
      "metadata": {
        "id": "Scojqoq6ePLk"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = np.zeros((len(word2id), 300))\n",
        "for word, i in word2id.items():\n",
        "    if word == 'PAD':\n",
        "        continue   \n",
        "    try:\n",
        "        weights[i] = ft[word]    \n",
        "    except KeyError:\n",
        "        weights[i] = np.random.normal(0,0.1,300)"
      ],
      "metadata": {
        "id": "ZxX8BGdBgIjY"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_FT(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=300):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.embedding.from_pretrained(torch.tensor(weights), freeze=True)\n",
        "        self.bigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=2, padding='same')\n",
        "        self.trigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=80, kernel_size=3, padding='same')\n",
        "        self.lastconv = nn.Conv1d(in_channels=180, out_channels=150, kernel_size=2, padding='same')\n",
        "        self.hidden = nn.Linear(in_features=150, out_features=1)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, word):\n",
        "        embedded = self.embedding(word)\n",
        "        embedded = embedded.transpose(1,2)\n",
        "        feature_map_bigrams = self.dropout(self.bigrams(embedded))\n",
        "        feature_map_trigrams = self.dropout(self.trigrams(embedded))\n",
        "        concat = torch.cat((feature_map_bigrams, feature_map_trigrams), 1)\n",
        "        feature_map_last = self.dropout(self.lastconv(concat))\n",
        "        pooling = feature_map_last.max(2)[0]\n",
        "        logits = self.hidden(pooling) \n",
        "        logits = self.out(logits)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "eoT6w4OXgNIH"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN_FT(len(word2id))\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "criterion = nn.BCELoss()  \n",
        "\n",
        "model = model.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)"
      ],
      "metadata": {
        "id": "Esj226ztkywG"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(5):\n",
        "    print(f'\\nepoch {i}')\n",
        "    epoch_loss = training(model, train_iterator, optimizer, criterion)\n",
        "    print('epoch loss on train:', epoch_loss)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\ntest evaluation:')\n",
        "    f1_on_test, epoch_loss_on_test = evaluate(model, test_iterator, criterion)\n",
        "    print('epoch loss on test:', epoch_loss_on_test)\n",
        "    print('F1 on test:', f1_on_test)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UprtqFYk-1Q",
        "outputId": "7815bc5b-5556-46f9-d3fa-2cc8771e19b5"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 0\n",
            "epoch loss on train: 0.6227839518798871\n",
            "\n",
            "test evaluation:\n",
            "epoch loss on test: 0.5767073671023051\n",
            "F1 on test: 0.6986798935466343\n",
            "\n",
            "epoch 1\n",
            "epoch loss on train: 0.5517790782987402\n",
            "\n",
            "test evaluation:\n",
            "epoch loss on test: 0.5376903878317939\n",
            "F1 on test: 0.7279510166909959\n",
            "\n",
            "epoch 2\n",
            "epoch loss on train: 0.5117080710577161\n",
            "\n",
            "test evaluation:\n",
            "epoch loss on test: 0.514421033859253\n",
            "F1 on test: 0.7459974884986877\n",
            "\n",
            "epoch 3\n",
            "epoch loss on train: 0.4800851465610976\n",
            "\n",
            "test evaluation:\n",
            "epoch loss on test: 0.4952606174680922\n",
            "F1 on test: 0.7491783420244853\n",
            "\n",
            "epoch 4\n",
            "epoch loss on train: 0.4540397950102774\n",
            "\n",
            "test evaluation:\n",
            "epoch loss on test: 0.48092230757077536\n",
            "F1 on test: 0.7657908492618137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. CNN for words and symbols"
      ],
      "metadata": {
        "id": "3zA4tKPVsH53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_symbols = Counter()\n",
        "\n",
        "for words in dataset['tokenized']:\n",
        "    for word in words:\n",
        "        vocab_symbols.update(list(word))\n",
        "print('всего уникальных символов:', len(vocab_symbols))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7Xc5YOfrX0b",
        "outputId": "c9ee062b-86ff-400a-cbac-0f5c4a2129b2"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "всего уникальных символов: 367\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_vocab_symbols = set()\n",
        "\n",
        "for symbol in vocab_symbols:\n",
        "    if vocab_symbols[symbol] > 5:\n",
        "        filtered_vocab_symbols.add(symbol)\n",
        "print('уникальных символов, втретившихся больше 5 раз:', len(filtered_vocab_symbols))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfJW5eBGtDOi",
        "outputId": "76bf4c88-b333-4689-b427-669a53f0526f"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "уникальных символов, втретившихся больше 5 раз: 151\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "symbol2id = {'PAD':0}\n",
        "\n",
        "for symbol in filtered_vocab_symbols:\n",
        "    symbol2id[symbol] = len(symbol2id)\n",
        "\n",
        "id2symbol = {i:symbol for symbol, i in symbol2id.items()}"
      ],
      "metadata": {
        "id": "nCd-zH67tOpH"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TwitterDataset_WS(Dataset):\n",
        "    def __init__(self, dataset, word2id, symbol2id, DEVICE):\n",
        "        self.dataset = dataset['tokenized'].values\n",
        "        self.word2id = word2id\n",
        "        self.symbol2id = symbol2id\n",
        "        self.length = dataset.shape[0]\n",
        "        self.target = dataset['class'].values\n",
        "        self.device = DEVICE\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index): \n",
        "        words = self.dataset[index]\n",
        "        ids_w = torch.LongTensor([self.word2id[word] for word in words if word in self.word2id])\n",
        "        symbols = [symb for word in self.dataset[index] for symb in word]\n",
        "        ids_s = torch.LongTensor([self.symbol2id[symbol] for symbol in symbols if symbol in self.symbol2id])\n",
        "        y = [self.target[index]]\n",
        "        return ids_w, ids_s, y\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        ids_w, ids_s, y = list(zip(*batch))\n",
        "        padded_ids_w = pad_sequence(ids_w, batch_first=True).to(self.device)\n",
        "        padded_ids_s = pad_sequence(ids_s, batch_first=True).to(self.device)\n",
        "        y = torch.Tensor(y).to(self.device)\n",
        "        return padded_ids_w, padded_ids_s, y"
      ],
      "metadata": {
        "id": "gl-3TujplE4p"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TwitterDataset_WS(train, word2id, symbol2id, DEVICE)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_iterator = DataLoader(train_dataset, collate_fn = train_dataset.collate_fn, sampler=train_sampler, batch_size=1024)"
      ],
      "metadata": {
        "id": "-_3mu2w1uwZv"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = TwitterDataset_WS(test, word2id, symbol2id, DEVICE)\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_iterator = DataLoader(test_dataset, collate_fn = test_dataset.collate_fn, sampler=test_sampler, batch_size=1024)"
      ],
      "metadata": {
        "id": "ZXs8p8LXvyYo"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (padded_ids_w, padded_ids_s, ys) in enumerate(train_iterator):\n",
        "    print(padded_ids_w)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wkeooH74NPr",
        "outputId": "39672cae-cfb2-43e3-a27c-1a7d3bc413fb"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[14525,  1458,  5809,  ...,     0,     0,     0],\n",
            "        [ 2256,  5809,  7242,  ...,     0,     0,     0],\n",
            "        [ 1895,  4911, 28460,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [ 1818, 16127, 10889,  ...,     0,     0,     0],\n",
            "        [19384, 31385, 31606,  ...,     0,     0,     0],\n",
            "        [12790, 22873, 28227,  ...,     0,     0,     0]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def training2(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0 \n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for i, (words, symbols, ys) in enumerate(iterator):\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(words, symbols)\n",
        "        loss = criterion(preds, ys)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    return  epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "__q3s1RF_vs_"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate2(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_metric = 0\n",
        "    model.eval() \n",
        "    with torch.no_grad():\n",
        "        for i, (words, symbols, ys) in enumerate(iterator):   \n",
        "            preds = model(words, symbols)\n",
        "            loss = criterion(preds, ys)\n",
        "            epoch_loss += loss.item()\n",
        "            batch_metric = f1(preds.round().long(), ys.long(), ignore_index=0)\n",
        "            epoch_metric += batch_metric.item()\n",
        "        \n",
        "    return epoch_metric / len(iterator), epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "g2D35Oq4_psw"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_WS_DO(nn.Module):\n",
        "    def __init__(self, vocab_size_words, embedding_dim_words,\n",
        "                 vocab_size_symbols, embedding_dim_symbols):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size_words, embedding_dim_words)\n",
        "        self.embedding.from_pretrained(torch.tensor(weights), freeze=True)\n",
        "        self.hidden = nn.Linear(in_features=300, out_features=100)\n",
        "\n",
        "        self.embedding_symbols = nn.Embedding(vocab_size_symbols, embedding_dim_symbols)\n",
        "        self.bigrams = nn.Conv1d(in_channels=embedding_dim_symbols, out_channels=80, kernel_size=2, padding='same')\n",
        "        self.trigrams = nn.Conv1d(in_channels=embedding_dim_symbols, out_channels=100, kernel_size=3, padding='same')\n",
        "        self.second_hidden = nn.Linear(in_features=280, out_features=1)\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, word, symbols):\n",
        "        embedded_words = self.embedding(word)\n",
        "        embedded_words = embedded_words.max(1)[0]\n",
        "        X = self.hidden(embedded_words)\n",
        "\n",
        "        embedded_symbols = self.embedding_symbols(symbols)\n",
        "        embedded_symbols = embedded_symbols.transpose(1,2)\n",
        "        feature_map_bigrams = self.dropout(self.bigrams(embedded_symbols))\n",
        "        feature_map_trigrams = self.dropout(self.trigrams(embedded_symbols))\n",
        "        pooling1 = feature_map_bigrams.max(2)[0] \n",
        "        pooling2 = feature_map_trigrams.max(2)[0]\n",
        "        concat = torch.cat((pooling1, pooling2), 1)\n",
        "\n",
        "        concat = torch.cat((concat, X), 1)\n",
        "\n",
        "        logits = self.second_hidden(concat) \n",
        "        logits = self.out(logits)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "K5XtPWwO6Ies"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN_WS_DO(len(word2id), 300, len(symbol2id), 15)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "criterion = nn.BCELoss()  \n",
        "\n",
        "model = model.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)\n",
        "\n",
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(5):\n",
        "    print(f'\\nepoch {i}')\n",
        "    epoch_loss = training2(model, train_iterator, optimizer, criterion)\n",
        "    print('epoch loss on train:', epoch_loss)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\ntest evaluation:')\n",
        "    f1_on_test, epoch_loss_on_test = evaluate2(model, test_iterator, criterion)\n",
        "    print('epoch loss on test:', epoch_loss_on_test)\n",
        "    print('F1 on test:', f1_on_test)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_BtuCFRAozT",
        "outputId": "41030248-db1a-4077-a7ee-690a598af5ab"
      },
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 0\n",
            "epoch loss on train: 0.6523520427473476\n",
            "\n",
            "test evaluation:\n",
            "epoch loss on test: 0.6160269962416755\n",
            "F1 on test: 0.7055394755469429\n",
            "\n",
            "epoch 1\n",
            "epoch loss on train: 0.5728454201408987\n",
            "\n",
            "test evaluation:\n",
            "epoch loss on test: 0.565004743470086\n",
            "F1 on test: 0.6557340463002522\n",
            "\n",
            "epoch 2\n",
            "epoch loss on train: 0.5282727764563614\n",
            "\n",
            "test evaluation:\n",
            "epoch loss on test: 0.5431005941496955\n",
            "F1 on test: 0.6726143015755548\n",
            "\n",
            "epoch 3\n",
            "epoch loss on train: 0.4957212819142288\n",
            "\n",
            "test evaluation:\n",
            "epoch loss on test: 0.5131849845250448\n",
            "F1 on test: 0.7221371690432231\n",
            "\n",
            "epoch 4\n",
            "epoch loss on train: 0.46915717372733556\n",
            "\n",
            "test evaluation:\n",
            "epoch loss on test: 0.4983571635352241\n",
            "F1 on test: 0.7401250110732185\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Улучшение: убран Dropout, изменены параметры in/out_chanels и размер символьного эмбеддинга."
      ],
      "metadata": {
        "id": "Jn_ikJvtB0dv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_WS(nn.Module):\n",
        "    def __init__(self, vocab_size_words, embedding_dim_words,\n",
        "                 vocab_size_symbols, embedding_dim_symbols):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size_words, embedding_dim_words)\n",
        "        self.embedding.from_pretrained(torch.tensor(weights), freeze=True)\n",
        "        self.hidden = nn.Linear(in_features=300, out_features=50)\n",
        "\n",
        "        self.embedding_symbols = nn.Embedding(vocab_size_symbols, embedding_dim_symbols)\n",
        "        self.bigrams = nn.Conv1d(in_channels=embedding_dim_symbols, out_channels=50, kernel_size=2, padding='same')\n",
        "        self.trigrams = nn.Conv1d(in_channels=embedding_dim_symbols, out_channels=50, kernel_size=3, padding='same')\n",
        "        self.second_hidden = nn.Linear(in_features=150, out_features=1)\n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, word, symbols):\n",
        "        embedded_words = self.embedding(word)\n",
        "        embedded_words = embedded_words.max(1)[0]\n",
        "        X = self.hidden(embedded_words)\n",
        "\n",
        "        embedded_symbols = self.embedding_symbols(symbols)\n",
        "        embedded_symbols = embedded_symbols.transpose(1,2)\n",
        "        feature_map_bigrams = self.bigrams(embedded_symbols)\n",
        "        feature_map_trigrams = self.trigrams(embedded_symbols)\n",
        "        pooling1 = feature_map_bigrams.max(2)[0] \n",
        "        pooling2 = feature_map_trigrams.max(2)[0]\n",
        "        concat = torch.cat((pooling1, pooling2), 1)\n",
        "\n",
        "        concat = torch.cat((concat, X), 1)\n",
        "\n",
        "        logits = self.second_hidden(concat) \n",
        "        logits = self.out(logits)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "Ez6YXBIF0ZkC"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN_WS(len(word2id), 300, len(symbol2id), 15)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "criterion = nn.BCELoss()  \n",
        "\n",
        "model = model.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)\n",
        "\n",
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(5):\n",
        "    print(f'\\nepoch {i}')\n",
        "    epoch_loss = training2(model, train_iterator, optimizer, criterion)\n",
        "    print('epoch loss on train:', epoch_loss)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\ntest evaluation:')\n",
        "    f1_on_test, epoch_loss_on_test = evaluate2(model, test_iterator, criterion)\n",
        "    print('epoch loss on test:', epoch_loss_on_test)\n",
        "    print('F1 on test:', f1_on_test)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVGBJzul6InP",
        "outputId": "c3732c11-c280-40b8-d67d-9b63ead17327"
      },
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 0\n",
            "epoch loss on train: 0.6546003433425774\n",
            "\n",
            "test evaluation:\n",
            "epoch loss on test: 0.6117788314819336\n",
            "F1 on test: 0.646149484316508\n",
            "\n",
            "epoch 1\n",
            "epoch loss on train: 0.5774652757001727\n",
            "\n",
            "test evaluation:\n",
            "epoch loss on test: 0.5631250182787577\n",
            "F1 on test: 0.6986657116148207\n",
            "\n",
            "epoch 2\n",
            "epoch loss on train: 0.5345779094803199\n",
            "\n",
            "test evaluation:\n",
            "epoch loss on test: 0.5468138708008661\n",
            "F1 on test: 0.7337905367215475\n",
            "\n",
            "epoch 3\n",
            "epoch loss on train: 0.5021812693121728\n",
            "\n",
            "test evaluation:\n",
            "epoch loss on test: 0.5368823170661926\n",
            "F1 on test: 0.7483912971284654\n",
            "\n",
            "epoch 4\n",
            "epoch loss on train: 0.47435664745529044\n",
            "\n",
            "test evaluation:\n",
            "epoch loss on test: 0.5165335496266683\n",
            "F1 on test: 0.7569814072714911\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "l7G7TvoODGlG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "HW_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}